{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset from Kaggle\n",
        "**Hurricanes and Typhoons, 1851-2014** by National Oceanic and Atmospheric Administration (NOAA)\n",
        "\n",
        "Dataset includes two separate CSV files. We combine into one CSV file and apply some preprocessing."
      ],
      "metadata": {
        "id": "edQlAXjWE83s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7ExZ5_6hqOJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# data file paths in repo\n",
        "DATA_DIR = Path(\"data\")\n",
        "ATLANTIC_PATH = DATA_DIR / \"atlantic.csv\"\n",
        "PACIFIC_PATH  = DATA_DIR / \"pacific.csv\"\n",
        "STORMS_PATH   = DATA_DIR / \"storms.csv\"\n",
        "\n",
        "# load databases and combine\n",
        "df_atl = pd.read_csv(ATLANTIC_PATH)\n",
        "df_pac = pd.read_csv(PACIFIC_PATH)\n",
        "df = pd.concat([df_atl, df_pac], ignore_index=True)\n",
        "\n",
        "# drop columns with missing values\n",
        "cols_to_drop = [\n",
        "  \"Event\",\n",
        "  \"Minimum Pressure\",\n",
        "  \"Low Wind NE\", \"Low Wind SE\", \"Low Wind SW\", \"Low Wind NW\",\n",
        "  \"Moderate Wind NE\", \"Moderate Wind SE\", \"Moderate Wind SW\", \"Moderate Wind NW\",\n",
        "  \"High Wind NE\", \"High Wind SE\", \"High Wind SW\", \"High Wind NW\"\n",
        "]\n",
        "df = df.drop(columns=cols_to_drop)\n",
        "\n",
        "# convert Lat/Long to floats\n",
        "def parse_lat(val):\n",
        "  if pd.isna(val):\n",
        "    return np.nan\n",
        "  s = str(val).strip()\n",
        "  if s == \"\" or s == \"-999\":\n",
        "    return np.nan\n",
        "  direction = s[-1].upper()\n",
        "  number = float(s[:-1])\n",
        "  if direction == \"S\":\n",
        "    number = -number\n",
        "  return number\n",
        "\n",
        "def parse_lon(val):\n",
        "  if pd.isna(val):\n",
        "    return np.nan\n",
        "  s = str(val).strip()\n",
        "  if s == \"\" or s == \"-999\":\n",
        "    return np.nan\n",
        "  direction = s[-1].upper()\n",
        "  number = float(s[:-1])\n",
        "  if direction == \"W\":\n",
        "    number = -number\n",
        "  return number\n",
        "\n",
        "df[\"Latitude\"] = df[\"Latitude\"].apply(parse_lat)\n",
        "df[\"Longitude\"] = df[\"Longitude\"].apply(parse_lon)\n",
        "# normalize longitude\n",
        "df[\"Longitude\"] = ((df[\"Longitude\"] + 180) % 360) - 180\n",
        "\n",
        "# normalize storm status to int\n",
        "status_map = {\n",
        "  \"DB\": 0,\n",
        "  \"LO\": 1,\n",
        "  \"SD\": 2,\n",
        "  \"TD\": 3,\n",
        "  \"WV\": 3,\n",
        "  \"SS\": 4,\n",
        "  \"TS\": 5,\n",
        "  \"HU\": 6,\n",
        "  \"ST\": 6,\n",
        "  \"EX\": 7,\n",
        "  \"ET\": 7,\n",
        "  \"PT\": 8\n",
        "}\n",
        "df[\"Status\"] = df[\"Status\"].astype(str).str.strip().map(status_map)\n",
        "\n",
        "# save combined file\n",
        "df.to_csv(STORMS_PATH, index=False)\n",
        "print(f\"Saved cleaned dataset to {STORMS_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Preprocessing"
      ],
      "metadata": {
        "id": "SO0xnN3SGafl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "STORMS_PATH = DATA_DIR / \"storms.csv\"\n",
        "\n",
        "df = pd.read_csv(STORMS_PATH)\n",
        "\n",
        "# combine Date and Time to string and converting them to datetime values\n",
        "df[\"Date\"] = df[\"Date\"].astype(str).str.zfill(8)   # YYYYMMDD\n",
        "df[\"Time\"] = df[\"Time\"].astype(str).str.zfill(4)   # HHMM\n",
        "df[\"Datetime\"] = pd.to_datetime(df[\"Date\"] + df[\"Time\"],\n",
        "                                format=\"%Y%m%d%H%M\")\n",
        "\n",
        "# sort rows by storm ID and datetime\n",
        "df = df.sort_values([\"ID\", \"Datetime\"]).reset_index(drop=True)\n",
        "\n",
        "# calculate hours since storm began for each row\n",
        "genesis_time = df.groupby(\"ID\")[\"Datetime\"].transform(\"min\")\n",
        "df[\"hours_since_genesis\"] = (df[\"Datetime\"] - genesis_time).dt.total_seconds() / 3600.0\n",
        "\n",
        "# use the last 4 rows (24 hours) to predict\n",
        "input_steps = 4\n",
        "\n",
        "# predict the next 4 rows (24 hours)\n",
        "forecast_steps = 4\n",
        "\n",
        "# columns used for prediction model\n",
        "feature_cols = [\"Latitude\", \"Longitude\", \"Maximum Wind\", \"Status\", \"hours_since_genesis\"]\n",
        "\n",
        "X_list = []\n",
        "y_deltas_list = []\n",
        "y_alive_list = []\n",
        "seq_ids = []\n",
        "\n",
        "for storm_id, g in df.groupby(\"ID\"):\n",
        "  g = g.sort_values(\"Datetime\").reset_index(drop=True)\n",
        "\n",
        "  # if storm lasted for less than 5 rows, skip it\n",
        "  if len(g) < input_steps + 1:\n",
        "    continue\n",
        "\n",
        "  data = g[feature_cols].values\n",
        "  # lats/lons used to build target deltas\n",
        "  lats = g[\"Latitude\"].values\n",
        "  lons = g[\"Longitude\"].values\n",
        "\n",
        "  last_idx = len(g) - 1\n",
        "  max_start = len(g) - input_steps - 1\n",
        "  if max_start < 0:\n",
        "    continue\n",
        "\n",
        "  for start in range(max_start + 1):\n",
        "    end_input_idx = start + input_steps - 1\n",
        "\n",
        "    # 4x5 matrix input\n",
        "    X_seq = data[start:start + input_steps]\n",
        "\n",
        "    # last storm coordinates before prediction\n",
        "    base_lat = lats[end_input_idx]\n",
        "    base_lon = lons[end_input_idx]\n",
        "\n",
        "    y_deltas = []\n",
        "    y_alive = []\n",
        "\n",
        "    # storm movement/status after \"prediction point\"\n",
        "    for h in range(1, forecast_steps + 1):\n",
        "      future_idx = end_input_idx + h\n",
        "\n",
        "      if future_idx <= last_idx:\n",
        "\n",
        "        lat_f = lats[future_idx]\n",
        "        lon_f = lons[future_idx]\n",
        "\n",
        "        # how far the storm moved in degrees\n",
        "        dlat = lat_f - base_lat\n",
        "        dlon = lon_f - base_lon\n",
        "\n",
        "        y_deltas.append((dlat, dlon))\n",
        "        y_alive.append(1.0)\n",
        "\n",
        "      else:\n",
        "        # if storm dissipated, placeholder delta which are masked in the loss\n",
        "        y_deltas.append((0.0, 0.0))\n",
        "        y_alive.append(0.0)\n",
        "\n",
        "    X_list.append(X_seq)\n",
        "    y_deltas_list.append(y_deltas)\n",
        "    y_alive_list.append(y_alive)\n",
        "    seq_ids.append(storm_id)\n",
        "\n",
        "# convert lists to NumPy arrays\n",
        "X = np.array(X_list, dtype=np.float32)\n",
        "y_deltas = np.array(y_deltas_list, dtype=np.float32)\n",
        "y_alive = np.array(y_alive_list, dtype=np.float32)\n",
        "seq_ids = np.array(seq_ids)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y_deltas shape:\", y_deltas.shape)\n",
        "print(\"y_alive shape:\", y_alive.shape)"
      ],
      "metadata": {
        "id": "Zi7phqZVYQOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Data\n",
        "80% training, 10% Testing, 10% Validation"
      ],
      "metadata": {
        "id": "1_zexWEwMqiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting by storms not rows\n",
        "storm_ids = np.unique(seq_ids)\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(storm_ids)\n",
        "\n",
        "# 80-10-10 split for training-validation-testing\n",
        "n = len(storm_ids)\n",
        "n_train = int(0.8 * n)\n",
        "n_val = int(0.1 * n)\n",
        "\n",
        "train_ids = storm_ids[:n_train]\n",
        "val_ids   = storm_ids[n_train:n_train + n_val]\n",
        "test_ids  = storm_ids[n_train + n_val:]\n",
        "\n",
        "# masks for split\n",
        "train_mask = np.isin(seq_ids, train_ids)\n",
        "val_mask   = np.isin(seq_ids, val_ids)\n",
        "test_mask  = np.isin(seq_ids, test_ids)\n",
        "\n",
        "X_train      = X[train_mask]\n",
        "y_deltas_train = y_deltas[train_mask]\n",
        "y_alive_train  = y_alive[train_mask]\n",
        "\n",
        "X_val        = X[val_mask]\n",
        "y_deltas_val = y_deltas[val_mask]\n",
        "y_alive_val  = y_alive[val_mask]\n",
        "\n",
        "X_test        = X[test_mask]\n",
        "y_deltas_test = y_deltas[test_mask]\n",
        "y_alive_test  = y_alive[test_mask]\n",
        "\n",
        "print(\"Total storms:\", len(storm_ids))\n",
        "print(\"Train storms:\", np.unique(seq_ids[train_mask]).shape[0])\n",
        "print(\"Val storms:  \", np.unique(seq_ids[val_mask]).shape[0])\n",
        "print(\"Test storms: \", np.unique(seq_ids[test_mask]).shape[0])\n",
        "print(\"X_train:\", X_train.shape,\n",
        "      \"y_deltas_train:\", y_deltas_train.shape,\n",
        "      \"y_alive_train:\", y_alive_train.shape)\n",
        "print(\"X_val:  \", X_val.shape,\n",
        "      \"y_deltas_val:\", y_deltas_val.shape,\n",
        "      \"y_alive_val:\", y_alive_val.shape)\n",
        "print(\"X_test: \", X_test.shape,\n",
        "      \"y_deltas_test:\", y_deltas_test.shape,\n",
        "      \"y_alive_test:\", y_alive_test.shape)"
      ],
      "metadata": {
        "id": "gZnQ4FettKbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training GRU Model"
      ],
      "metadata": {
        "id": "SaB-3VahM4CB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# convert NumPy arrays to PyTorch tensors\n",
        "X_train_t      = torch.from_numpy(X_train).float()\n",
        "y_deltas_train_t = torch.from_numpy(y_deltas_train).float()\n",
        "y_alive_train_t  = torch.from_numpy(y_alive_train).float()\n",
        "\n",
        "X_val_t        = torch.from_numpy(X_val).float()\n",
        "y_deltas_val_t = torch.from_numpy(y_deltas_val).float()\n",
        "y_alive_val_t  = torch.from_numpy(y_alive_val).float()\n",
        "\n",
        "X_test_t        = torch.from_numpy(X_test).float()\n",
        "y_deltas_test_t = torch.from_numpy(y_deltas_test).float()\n",
        "y_alive_test_t  = torch.from_numpy(y_alive_test).float()\n",
        "\n",
        "# mean and standard deviation of training input\n",
        "feat_mean = X_train_t.mean(dim=(0, 1), keepdim=True)\n",
        "feat_std  = X_train_t.std(dim=(0, 1), keepdim=True) + 1e-6\n",
        "\n",
        "# standardize data using mean/std\n",
        "X_train_t = (X_train_t - feat_mean) / feat_std\n",
        "X_val_t   = (X_val_t   - feat_mean) / feat_std\n",
        "X_test_t  = (X_test_t  - feat_mean) / feat_std\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# wrap tensors into TensorDatasets\n",
        "train_ds = TensorDataset(X_train_t, y_deltas_train_t, y_alive_train_t)\n",
        "val_ds   = TensorDataset(X_val_t,   y_deltas_val_t,   y_alive_val_t)\n",
        "test_ds  = TensorDataset(X_test_t,  y_deltas_test_t,  y_alive_test_t)\n",
        "\n",
        "# DataLoader objects\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Gated Recurrent Unit (GRU) model\n",
        "class GRUTrackModel(nn.Module):\n",
        "  def __init__(self, input_dim=5, hidden_dim=256, num_layers=2,\n",
        "               forecast_steps=4):\n",
        "    super().__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.forecast_steps = forecast_steps\n",
        "\n",
        "    self.gru = nn.GRU(\n",
        "      input_size=input_dim,\n",
        "      hidden_size=hidden_dim,\n",
        "      num_layers=num_layers,\n",
        "      batch_first=True\n",
        "    )\n",
        "\n",
        "    # maps the final GRU hidden state to outputs\n",
        "    self.fc_coords = nn.Linear(hidden_dim, forecast_steps * 2)\n",
        "    self.fc_alive = nn.Linear(hidden_dim, forecast_steps)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, h_n = self.gru(x)\n",
        "\n",
        "    # GRU output at last step\n",
        "    h_last = out[:, -1, :]\n",
        "\n",
        "    # predict next coordinates and whether storm is alive\n",
        "    coords_flat = self.fc_coords(h_last)\n",
        "    coords_pred = coords_flat.view(-1, self.forecast_steps, 2)\n",
        "    alive_logits = self.fc_alive(h_last)\n",
        "\n",
        "    return coords_pred, alive_logits\n",
        "\n",
        "# model dimensions\n",
        "model = GRUTrackModel(\n",
        "  input_dim=5,\n",
        "  hidden_dim=256,\n",
        "  num_layers=2,\n",
        "  forecast_steps=4\n",
        ").to(device)\n",
        "\n",
        "# Smooth L1 Loss Function for coordinates\n",
        "coord_loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n",
        "\n",
        "# Binary Cross-Entropy (BCE) with Logits Loss Function for storm alive status\n",
        "alive_loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# AdamW optimizer with MultiStepLR scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer,\n",
        "    milestones=[20],\n",
        "    gamma=0.3\n",
        ")\n",
        "\n",
        "# weights for multi-task loss\n",
        "lambda_coord = 1.0\n",
        "lambda_alive = 0.5\n",
        "\n",
        "# method to train one epoch\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  total_coord_loss = 0.0\n",
        "  total_alive_loss = 0.0\n",
        "  count = 0\n",
        "\n",
        "  # loop through each batch\n",
        "  for X_batch, y_coords_batch, y_alive_batch in loader:\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_coords_batch = y_coords_batch.to(device)\n",
        "    y_alive_batch = y_alive_batch.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # run model forward on current batch\n",
        "    coords_pred, alive_logits = model(X_batch)\n",
        "\n",
        "    # compute loss\n",
        "    alive_loss = alive_loss_fn(alive_logits, y_alive_batch)\n",
        "    coord_loss_per_elem = coord_loss_fn(coords_pred, y_coords_batch)\n",
        "    # build mask for alive status\n",
        "    alive_mask = (y_alive_batch > 0.5).unsqueeze(-1)\n",
        "    alive_mask = alive_mask.expand_as(coord_loss_per_elem)\n",
        "    masked_loss = coord_loss_per_elem * alive_mask.float()\n",
        "    # number of valid(alive) elements\n",
        "    denom = alive_mask.float().sum()\n",
        "    if denom > 0:\n",
        "      coord_loss = masked_loss.sum() / denom\n",
        "    else:\n",
        "      coord_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "    # combine the two tasks into one scalar loss\n",
        "    loss = lambda_coord * coord_loss + lambda_alive * alive_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # multiply each loss by batch_size to compute true average later\n",
        "    batch_size = X_batch.size(0)\n",
        "    total_loss += loss.item() * batch_size\n",
        "    total_coord_loss += coord_loss.item() * batch_size\n",
        "    total_alive_loss += alive_loss.item() * batch_size\n",
        "    count += batch_size\n",
        "\n",
        "  return (\n",
        "    total_loss / count,\n",
        "    total_coord_loss / count,\n",
        "    total_alive_loss / count,\n",
        "  )\n",
        "\n",
        "# evaluation function\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "  total_coord_loss = 0.0\n",
        "  total_alive_loss = 0.0\n",
        "  count = 0\n",
        "\n",
        "  # loop through each batch\n",
        "  for X_batch, y_coords_batch, y_alive_batch in loader:\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_coords_batch = y_coords_batch.to(device)\n",
        "    y_alive_batch = y_alive_batch.to(device)\n",
        "\n",
        "    # run model forward on current batch\n",
        "    coords_pred, alive_logits = model(X_batch)\n",
        "\n",
        "    # compute loss\n",
        "    alive_loss = alive_loss_fn(alive_logits, y_alive_batch)\n",
        "    coord_loss_per_elem = coord_loss_fn(coords_pred, y_coords_batch)\n",
        "    alive_mask = (y_alive_batch > 0.5).unsqueeze(-1)\n",
        "    alive_mask = alive_mask.expand_as(coord_loss_per_elem)\n",
        "    masked_loss = coord_loss_per_elem * alive_mask.float()\n",
        "    denom = alive_mask.float().sum()\n",
        "    if denom > 0:\n",
        "      coord_loss = masked_loss.sum() / denom\n",
        "    else:\n",
        "      coord_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "    # combine the two tasks into one scalar loss\n",
        "    loss = lambda_coord * coord_loss + lambda_alive * alive_loss\n",
        "\n",
        "    # average losses for the whole dataset\n",
        "    batch_size = X_batch.size(0)\n",
        "    total_loss += loss.item() * batch_size\n",
        "    total_coord_loss += coord_loss.item() * batch_size\n",
        "    total_alive_loss += alive_loss.item() * batch_size\n",
        "    count += batch_size\n",
        "\n",
        "  return (\n",
        "    total_loss / count,\n",
        "    total_coord_loss / count,\n",
        "    total_alive_loss / count,\n",
        "  )\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "# training loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  train_loss, train_coord_loss, train_alive_loss = train_one_epoch(\n",
        "    model, train_loader, optimizer, device\n",
        "  )\n",
        "  val_loss, val_coord_loss, val_alive_loss = evaluate(\n",
        "    model, val_loader, device\n",
        "  )\n",
        "  scheduler.step()\n",
        "\n",
        "  print(\n",
        "    f\"Epoch {epoch:02d} | \"\n",
        "    f\"Train Loss: {train_loss:.4f} (coord {train_coord_loss:.4f}, alive {train_alive_loss:.4f}) | \"\n",
        "    f\"Val Loss: {val_loss:.4f} (coord {val_coord_loss:.4f}, alive {val_alive_loss:.4f})\"\n",
        "  )\n",
        "\n",
        "test_loss, test_coord_loss, test_alive_loss = evaluate(model, test_loader, device)\n",
        "print(\n",
        "  f\"Test Loss: {test_loss:.4f} (coord {test_coord_loss:.4f}, alive {test_alive_loss:.4f})\"\n",
        ")"
      ],
      "metadata": {
        "id": "wXemL-TkzyK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Model Accuacy"
      ],
      "metadata": {
        "id": "gtAnOPzjNCZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# storm alive status accuracy\n",
        "@torch.no_grad()\n",
        "def alive_accuracy_per_horizon(model, loader, device, forecast_steps=4):\n",
        "  model.eval()\n",
        "  correct = torch.zeros(forecast_steps)\n",
        "  total = torch.zeros(forecast_steps)\n",
        "\n",
        "  for X_batch, y_coords_batch, y_alive_batch in loader:\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_alive_batch = y_alive_batch.to(device)\n",
        "\n",
        "    _, alive_logits = model(X_batch)\n",
        "    alive_pred = (torch.sigmoid(alive_logits) > 0.5).float()\n",
        "\n",
        "    for h in range(forecast_steps):\n",
        "      correct[h] += (alive_pred[:, h] == y_alive_batch[:, h]).sum().item()\n",
        "      total[h] += y_alive_batch[:, h].numel()\n",
        "\n",
        "  acc_per_h = (correct / total).tolist()\n",
        "  return acc_per_h\n",
        "\n",
        "# haversine formula to calculate distance between two points on sphere\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "  lat1, lon1, lat2, lon2 = map(torch.deg2rad, [lat1, lon1, lat2, lon2])\n",
        "  dlat = lat2 - lat1\n",
        "  dlon = lon2 - lon1\n",
        "  a = torch.sin(dlat/2)**2 + torch.cos(lat1)*torch.cos(lat2)*torch.sin(dlon/2)**2\n",
        "  c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
        "  return 6371.0 * c  # km\n",
        "\n",
        "# coordinate accruacy (by error)\n",
        "@torch.no_grad()\n",
        "def coordinate_error_km_per_horizon_delta(model, loader, device,\n",
        "                                          forecast_steps=4,\n",
        "                                          only_when_alive=True):\n",
        "  model.eval()\n",
        "  total_dist = torch.zeros(forecast_steps)\n",
        "  total_points = torch.zeros(forecast_steps)\n",
        "\n",
        "  for X_batch, y_deltas_batch, y_alive_batch in loader:\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_deltas_batch = y_deltas_batch.to(device)\n",
        "    y_alive_batch  = y_alive_batch.to(device)\n",
        "\n",
        "    deltas_pred, _ = model(X_batch)\n",
        "\n",
        "    lat_last = X_batch[:, -1, 0] * feat_std[0, 0, 0] + feat_mean[0, 0, 0]\n",
        "    lon_last = X_batch[:, -1, 1] * feat_std[0, 0, 1] + feat_mean[0, 0, 1]\n",
        "\n",
        "    for h in range(forecast_steps):\n",
        "      # predicted deltas\n",
        "      dlat_pred = deltas_pred[:, h, 0]\n",
        "      dlon_pred = deltas_pred[:, h, 1]\n",
        "\n",
        "      # true deltas\n",
        "      dlat_true = y_deltas_batch[:, h, 0]\n",
        "      dlon_true = y_deltas_batch[:, h, 1]\n",
        "\n",
        "      # convert deltas back to coordinates\n",
        "      lat_pred = lat_last + dlat_pred\n",
        "      lon_pred = lon_last + dlon_pred\n",
        "\n",
        "      lat_true = lat_last + dlat_true\n",
        "      lon_true = lon_last + dlon_true\n",
        "\n",
        "      dist = haversine(lat_pred, lon_pred, lat_true, lon_true)\n",
        "\n",
        "      if only_when_alive:\n",
        "        mask = (y_alive_batch[:, h] > 0.5)\n",
        "        if mask.sum() == 0:\n",
        "          continue\n",
        "        dist = dist[mask]\n",
        "\n",
        "      total_dist[h]  += dist.sum().item()\n",
        "      total_points[h] += dist.numel()\n",
        "\n",
        "  mean_err = (total_dist / total_points).tolist()\n",
        "  return mean_err\n",
        "\n",
        "acc_test_h  = alive_accuracy_per_horizon(model, test_loader, device)\n",
        "err_test_h = coordinate_error_km_per_horizon_delta(model, test_loader, device)\n",
        "print(\"Alive Status Accuracy: \", acc_test_h)\n",
        "print(\"Coordinate Error (km):\", err_test_h)"
      ],
      "metadata": {
        "id": "AkwjJO4t2olr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline for Comparison\n",
        "Persisetent baseline assumes future position of the storm will be the same as the current position. It assumes zero motion from the storm."
      ],
      "metadata": {
        "id": "TBoOdHCbOT-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def persistence_error_km_per_horizon(loader, device, forecast_steps=4, only_when_alive=True):\n",
        "  total_dist = torch.zeros(forecast_steps)\n",
        "  total_points = torch.zeros(forecast_steps)\n",
        "\n",
        "  for X_batch, y_coords_batch, y_alive_batch in loader:\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_coords_batch = y_coords_batch.to(device)\n",
        "    y_alive_batch = y_alive_batch.to(device)\n",
        "\n",
        "    lat_last = X_batch[:, -1, 0] * feat_std[0, 0, 0] + feat_mean[0, 0, 0]\n",
        "    lon_last = X_batch[:, -1, 1] * feat_std[0, 0, 1] + feat_mean[0, 0, 1]\n",
        "\n",
        "    for h in range(forecast_steps):\n",
        "      lat_true = y_coords_batch[:, h, 0]\n",
        "      lon_true = y_coords_batch[:, h, 1]\n",
        "\n",
        "      # baseline: predict same as last input\n",
        "      lat_pred = lat_last\n",
        "      lon_pred = lon_last\n",
        "\n",
        "      dist = haversine(lat_pred, lon_pred, lat_true, lon_true)\n",
        "\n",
        "      if only_when_alive:\n",
        "        mask = (y_alive_batch[:, h] > 0.5)\n",
        "        if mask.sum() == 0:\n",
        "          continue\n",
        "        dist = dist[mask]\n",
        "\n",
        "      total_dist[h] += dist.sum().item()\n",
        "      total_points[h] += dist.numel()\n",
        "\n",
        "  return (total_dist / total_points).tolist()\n",
        "\n",
        "persistence_test_err = persistence_error_km_per_horizon(\n",
        "    test_loader,\n",
        "    device=device,\n",
        "    forecast_steps=4,\n",
        "    only_when_alive=True\n",
        ")\n",
        "\n",
        "print(\"Persistence baseline (km): \", persistence_test_err)"
      ],
      "metadata": {
        "id": "LvKjFJ4hOasT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}