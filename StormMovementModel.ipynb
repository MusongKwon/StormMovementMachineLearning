{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7ExZ5_6hqOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54363760-b864-4ef2-b120-17a5c6d961e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cleaned dataset to /content/storms.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# file paths in Google Colab Files\n",
        "atlantic_path = \"/content/atlantic.csv\"\n",
        "pacific_path  = \"/content/pacific.csv\"\n",
        "\n",
        "# load databases and combine\n",
        "df_atl = pd.read_csv(atlantic_path)\n",
        "df_pac = pd.read_csv(pacific_path)\n",
        "df = pd.concat([df_atl, df_pac], ignore_index=True)\n",
        "\n",
        "# drop columns with missing values\n",
        "cols_to_drop = [\n",
        "  \"Event\",\n",
        "  \"Minimum Pressure\",\n",
        "  \"Low Wind NE\", \"Low Wind SE\", \"Low Wind SW\", \"Low Wind NW\",\n",
        "  \"Moderate Wind NE\", \"Moderate Wind SE\", \"Moderate Wind SW\", \"Moderate Wind NW\",\n",
        "  \"High Wind NE\", \"High Wind SE\", \"High Wind SW\", \"High Wind NW\"\n",
        "]\n",
        "df = df.drop(columns=cols_to_drop)\n",
        "\n",
        "# convert Lat/Long to floats\n",
        "def parse_lat(val):\n",
        "  if pd.isna(val):\n",
        "    return np.nan\n",
        "  s = str(val).strip()\n",
        "  if s == \"\" or s == \"-999\":\n",
        "    return np.nan\n",
        "  direction = s[-1].upper()\n",
        "  number = float(s[:-1])\n",
        "  if direction == \"S\":\n",
        "    number = -number\n",
        "  return number\n",
        "\n",
        "def parse_lon(val):\n",
        "  if pd.isna(val):\n",
        "    return np.nan\n",
        "  s = str(val).strip()\n",
        "  if s == \"\" or s == \"-999\":\n",
        "    return np.nan\n",
        "  direction = s[-1].upper()\n",
        "  number = float(s[:-1])\n",
        "  if direction == \"W\":\n",
        "    number = -number\n",
        "  return number\n",
        "\n",
        "df[\"Latitude\"] = df[\"Latitude\"].apply(parse_lat)\n",
        "df[\"Longitude\"] = df[\"Longitude\"].apply(parse_lon)\n",
        "# normalize longitude\n",
        "df[\"Longitude\"] = ((df[\"Longitude\"] + 180) % 360) - 180\n",
        "\n",
        "# normalize storm status to int\n",
        "status_map = {\n",
        "  \"DB\": 0,\n",
        "  \"LO\": 1,\n",
        "  \"SD\": 2,\n",
        "  \"TD\": 3,\n",
        "  \"WV\": 3,\n",
        "  \"SS\": 4,\n",
        "  \"TS\": 5,\n",
        "  \"HU\": 6,\n",
        "  \"ST\": 6,\n",
        "  \"EX\": 7,\n",
        "  \"ET\": 7,\n",
        "  \"PT\": 8\n",
        "}\n",
        "df[\"Status\"] = df[\"Status\"].astype(str).str.strip().map(status_map)\n",
        "\n",
        "# save combined file\n",
        "output_path = \"/content/storms.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"Saved cleaned dataset to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# preprocess data\n",
        "path = \"/content/storms.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# combine Date and Time to string and converting them to datetime values\n",
        "df[\"Date\"] = df[\"Date\"].astype(str).str.zfill(8)   # YYYYMMDD\n",
        "df[\"Time\"] = df[\"Time\"].astype(str).str.zfill(4)   # HHMM\n",
        "df[\"Datetime\"] = pd.to_datetime(df[\"Date\"] + df[\"Time\"],\n",
        "                                format=\"%Y%m%d%H%M\")\n",
        "\n",
        "# sort rows by storm ID and datetime\n",
        "df = df.sort_values([\"ID\", \"Datetime\"]).reset_index(drop=True)\n",
        "\n",
        "# calculate hours since storm began for each row\n",
        "genesis_time = df.groupby(\"ID\")[\"Datetime\"].transform(\"min\")\n",
        "df[\"hours_since_genesis\"] = (df[\"Datetime\"] - genesis_time).dt.total_seconds() / 3600.0\n",
        "\n",
        "# use the last 4 rows (24 hours) to predict\n",
        "input_steps = 4\n",
        "\n",
        "# predict the next 4 rows (24 hours)\n",
        "forecast_steps = 4\n",
        "\n",
        "# columns used for prediction model\n",
        "feature_cols = [\"Latitude\", \"Longitude\", \"Maximum Wind\", \"Status\", \"hours_since_genesis\"]\n",
        "\n",
        "X_list = []\n",
        "y_deltas_list = []\n",
        "y_alive_list = []\n",
        "seq_ids = []\n",
        "\n",
        "for storm_id, g in df.groupby(\"ID\"):\n",
        "  g = g.sort_values(\"Datetime\").reset_index(drop=True)\n",
        "\n",
        "  # if storm lasted for less than 5 rows, skip it\n",
        "  if len(g) < input_steps + 1:\n",
        "    continue\n",
        "\n",
        "  data = g[feature_cols].values\n",
        "  # lats/lons used to build target deltas\n",
        "  lats = g[\"Latitude\"].values\n",
        "  lons = g[\"Longitude\"].values\n",
        "\n",
        "  last_idx = len(g) - 1\n",
        "  max_start = len(g) - input_steps - 1\n",
        "  if max_start < 0:\n",
        "    continue\n",
        "\n",
        "  for start in range(max_start + 1):\n",
        "    end_input_idx = start + input_steps - 1\n",
        "\n",
        "    # 4x5 matrix input\n",
        "    X_seq = data[start:start + input_steps]\n",
        "\n",
        "    # last storm coordinates before prediction\n",
        "    base_lat = lats[end_input_idx]\n",
        "    base_lon = lons[end_input_idx]\n",
        "\n",
        "    y_deltas = []\n",
        "    y_alive = []\n",
        "\n",
        "    # storm movement/status after \"prediction point\"\n",
        "    for h in range(1, forecast_steps + 1):\n",
        "      future_idx = end_input_idx + h\n",
        "\n",
        "      if future_idx <= last_idx:\n",
        "\n",
        "        lat_f = lats[future_idx]\n",
        "        lon_f = lons[future_idx]\n",
        "\n",
        "        # how far the storm moved in degrees\n",
        "        dlat = lat_f - base_lat\n",
        "        dlon = lon_f - base_lon\n",
        "\n",
        "        y_deltas.append((dlat, dlon))\n",
        "        y_alive.append(1.0)\n",
        "\n",
        "      else:\n",
        "        # if storm dissipated, placeholder delta which are masked in the loss\n",
        "        y_deltas.append((0.0, 0.0))\n",
        "        y_alive.append(0.0)\n",
        "\n",
        "    X_list.append(X_seq)\n",
        "    y_deltas_list.append(y_deltas)\n",
        "    y_alive_list.append(y_alive)\n",
        "    seq_ids.append(storm_id)\n",
        "\n",
        "# convert lists to NumPy arrays\n",
        "X = np.array(X_list, dtype=np.float32)\n",
        "y_deltas = np.array(y_deltas_list, dtype=np.float32)\n",
        "y_alive = np.array(y_alive_list, dtype=np.float32)\n",
        "seq_ids = np.array(seq_ids)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y_deltas shape:\", y_deltas.shape)\n",
        "print(\"y_alive shape:\", y_alive.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi7phqZVYQOc",
        "outputId": "d0da4ddc-1554-44db-a999-e0a6761c2065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (63885, 4, 5)\n",
            "y_deltas shape: (63885, 4, 2)\n",
            "y_alive shape: (63885, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting by storms not rows\n",
        "storm_ids = np.unique(seq_ids)\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(storm_ids)\n",
        "\n",
        "# 80-10-10 split for training-validation-testing\n",
        "n = len(storm_ids)\n",
        "n_train = int(0.8 * n)\n",
        "n_val = int(0.1 * n)\n",
        "\n",
        "train_ids = storm_ids[:n_train]\n",
        "val_ids   = storm_ids[n_train:n_train + n_val]\n",
        "test_ids  = storm_ids[n_train + n_val:]\n",
        "\n",
        "# masks for split\n",
        "train_mask = np.isin(seq_ids, train_ids)\n",
        "val_mask   = np.isin(seq_ids, val_ids)\n",
        "test_mask  = np.isin(seq_ids, test_ids)\n",
        "\n",
        "X_train      = X[train_mask]\n",
        "y_deltas_train = y_deltas[train_mask]\n",
        "y_alive_train  = y_alive[train_mask]\n",
        "\n",
        "X_val        = X[val_mask]\n",
        "y_deltas_val = y_deltas[val_mask]\n",
        "y_alive_val  = y_alive[val_mask]\n",
        "\n",
        "X_test        = X[test_mask]\n",
        "y_deltas_test = y_deltas[test_mask]\n",
        "y_alive_test  = y_alive[test_mask]\n",
        "\n",
        "print(\"Total storms:\", len(storm_ids))\n",
        "print(\"Train storms:\", np.unique(seq_ids[train_mask]).shape[0])\n",
        "print(\"Val storms:  \", np.unique(seq_ids[val_mask]).shape[0])\n",
        "print(\"Test storms: \", np.unique(seq_ids[test_mask]).shape[0])\n",
        "print(\"X_train:\", X_train.shape,\n",
        "      \"y_deltas_train:\", y_deltas_train.shape,\n",
        "      \"y_alive_train:\", y_alive_train.shape)\n",
        "print(\"X_val:  \", X_val.shape,\n",
        "      \"y_deltas_val:\", y_deltas_val.shape,\n",
        "      \"y_alive_val:\", y_alive_val.shape)\n",
        "print(\"X_test: \", X_test.shape,\n",
        "      \"y_deltas_test:\", y_deltas_test.shape,\n",
        "      \"y_alive_test:\", y_alive_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZnQ4FettKbQ",
        "outputId": "65696577-75c2-4894-fbd2-4e4343063b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total storms: 2812\n",
            "Train storms: 2249\n",
            "Val storms:   281\n",
            "Test storms:  282\n",
            "X_train: (50834, 4, 5) y_deltas_train: (50834, 4, 2) y_alive_train: (50834, 4)\n",
            "X_val:   (6967, 4, 5) y_deltas_val: (6967, 4, 2) y_alive_val: (6967, 4)\n",
            "X_test:  (6084, 4, 5) y_deltas_test: (6084, 4, 2) y_alive_test: (6084, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# convert NumPy arrays to PyTorch tensors\n",
        "X_train_t      = torch.from_numpy(X_train).float()\n",
        "y_deltas_train_t = torch.from_numpy(y_deltas_train).float()\n",
        "y_alive_train_t  = torch.from_numpy(y_alive_train).float()\n",
        "\n",
        "X_val_t        = torch.from_numpy(X_val).float()\n",
        "y_deltas_val_t = torch.from_numpy(y_deltas_val).float()\n",
        "y_alive_val_t  = torch.from_numpy(y_alive_val).float()\n",
        "\n",
        "X_test_t        = torch.from_numpy(X_test).float()\n",
        "y_deltas_test_t = torch.from_numpy(y_deltas_test).float()\n",
        "y_alive_test_t  = torch.from_numpy(y_alive_test).float()\n",
        "\n",
        "# mean and standard deviation of training input\n",
        "feat_mean = X_train_t.mean(dim=(0, 1), keepdim=True)\n",
        "feat_std  = X_train_t.std(dim=(0, 1), keepdim=True) + 1e-6\n",
        "\n",
        "# standardize data using mean/std\n",
        "X_train_t = (X_train_t - feat_mean) / feat_std\n",
        "X_val_t   = (X_val_t   - feat_mean) / feat_std\n",
        "X_test_t  = (X_test_t  - feat_mean) / feat_std\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# wrap tensors into TensorDatasets\n",
        "train_ds = TensorDataset(X_train_t, y_deltas_train_t, y_alive_train_t)\n",
        "val_ds   = TensorDataset(X_val_t,   y_deltas_val_t,   y_alive_val_t)\n",
        "test_ds  = TensorDataset(X_test_t,  y_deltas_test_t,  y_alive_test_t)\n",
        "\n",
        "# DataLoader objects\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Gated Recurrent Unit (GRU) model\n",
        "class GRUTrackModel(nn.Module):\n",
        "  def __init__(self, input_dim=5, hidden_dim=256, num_layers=2,\n",
        "               forecast_steps=4):\n",
        "    super().__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.forecast_steps = forecast_steps\n",
        "\n",
        "    self.gru = nn.GRU(\n",
        "      input_size=input_dim,\n",
        "      hidden_size=hidden_dim,\n",
        "      num_layers=num_layers,\n",
        "      batch_first=True\n",
        "    )\n",
        "\n",
        "    # maps the final GRU hidden state to outputs\n",
        "    self.fc_coords = nn.Linear(hidden_dim, forecast_steps * 2)\n",
        "    self.fc_alive = nn.Linear(hidden_dim, forecast_steps)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, h_n = self.gru(x)\n",
        "\n",
        "    # GRU output at last step\n",
        "    h_last = out[:, -1, :]\n",
        "\n",
        "    # predict next coordinates and whether storm is alive\n",
        "    coords_flat = self.fc_coords(h_last)\n",
        "    coords_pred = coords_flat.view(-1, self.forecast_steps, 2)\n",
        "    alive_logits = self.fc_alive(h_last)\n",
        "\n",
        "    return coords_pred, alive_logits\n",
        "\n",
        "# model dimensions\n",
        "model = GRUTrackModel(\n",
        "  input_dim=5,\n",
        "  hidden_dim=256,\n",
        "  num_layers=2,\n",
        "  forecast_steps=4\n",
        ").to(device)\n",
        "\n",
        "# Smooth L1 Loss Function for coordinates\n",
        "coord_loss_fn = nn.SmoothL1Loss(reduction=\"none\")\n",
        "\n",
        "# Binary Cross-Entropy (BCE) with Logits Loss Function for storm alive status\n",
        "alive_loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# AdamW optimizer with MultiStepLR scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer,\n",
        "    milestones=[20],\n",
        "    gamma=0.3\n",
        ")\n",
        "\n",
        "# weights for multi-task loss\n",
        "lambda_coord = 1.0\n",
        "lambda_alive = 0.5\n",
        "\n",
        "# method to train one epoch\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  total_coord_loss = 0.0\n",
        "  total_alive_loss = 0.0\n",
        "  count = 0\n",
        "\n",
        "  # loop through each batch\n",
        "  for X_batch, y_coords_batch, y_alive_batch in loader:\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_coords_batch = y_coords_batch.to(device)\n",
        "    y_alive_batch = y_alive_batch.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # run model forward on current batch\n",
        "    coords_pred, alive_logits = model(X_batch)\n",
        "\n",
        "    # compute loss\n",
        "    alive_loss = alive_loss_fn(alive_logits, y_alive_batch)\n",
        "    coord_loss_per_elem = coord_loss_fn(coords_pred, y_coords_batch)\n",
        "    # build mask for alive status\n",
        "    alive_mask = (y_alive_batch > 0.5).unsqueeze(-1)\n",
        "    alive_mask = alive_mask.expand_as(coord_loss_per_elem)\n",
        "    masked_loss = coord_loss_per_elem * alive_mask.float()\n",
        "    # number of valid(alive) elements\n",
        "    denom = alive_mask.float().sum()\n",
        "    if denom > 0:\n",
        "      coord_loss = masked_loss.sum() / denom\n",
        "    else:\n",
        "      coord_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "    # combine the two tasks into one scalar loss\n",
        "    loss = lambda_coord * coord_loss + lambda_alive * alive_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # multiply each loss by batch_size to compute true average later\n",
        "    batch_size = X_batch.size(0)\n",
        "    total_loss += loss.item() * batch_size\n",
        "    total_coord_loss += coord_loss.item() * batch_size\n",
        "    total_alive_loss += alive_loss.item() * batch_size\n",
        "    count += batch_size\n",
        "\n",
        "  return (\n",
        "    total_loss / count,\n",
        "    total_coord_loss / count,\n",
        "    total_alive_loss / count,\n",
        "  )\n",
        "\n",
        "# evaluation function\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "  total_coord_loss = 0.0\n",
        "  total_alive_loss = 0.0\n",
        "  count = 0\n",
        "\n",
        "  # loop through each batch\n",
        "  for X_batch, y_coords_batch, y_alive_batch in loader:\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_coords_batch = y_coords_batch.to(device)\n",
        "    y_alive_batch = y_alive_batch.to(device)\n",
        "\n",
        "    # run model forward on current batch\n",
        "    coords_pred, alive_logits = model(X_batch)\n",
        "\n",
        "    # compute loss\n",
        "    alive_loss = alive_loss_fn(alive_logits, y_alive_batch)\n",
        "    coord_loss_per_elem = coord_loss_fn(coords_pred, y_coords_batch)\n",
        "    alive_mask = (y_alive_batch > 0.5).unsqueeze(-1)\n",
        "    alive_mask = alive_mask.expand_as(coord_loss_per_elem)\n",
        "    masked_loss = coord_loss_per_elem * alive_mask.float()\n",
        "    denom = alive_mask.float().sum()\n",
        "    if denom > 0:\n",
        "      coord_loss = masked_loss.sum() / denom\n",
        "    else:\n",
        "      coord_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "    # combine the two tasks into one scalar loss\n",
        "    loss = lambda_coord * coord_loss + lambda_alive * alive_loss\n",
        "\n",
        "    # average losses for the whole dataset\n",
        "    batch_size = X_batch.size(0)\n",
        "    total_loss += loss.item() * batch_size\n",
        "    total_coord_loss += coord_loss.item() * batch_size\n",
        "    total_alive_loss += alive_loss.item() * batch_size\n",
        "    count += batch_size\n",
        "\n",
        "  return (\n",
        "    total_loss / count,\n",
        "    total_coord_loss / count,\n",
        "    total_alive_loss / count,\n",
        "  )\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "# training loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  train_loss, train_coord_loss, train_alive_loss = train_one_epoch(\n",
        "    model, train_loader, optimizer, device\n",
        "  )\n",
        "  val_loss, val_coord_loss, val_alive_loss = evaluate(\n",
        "    model, val_loader, device\n",
        "  )\n",
        "  scheduler.step()\n",
        "\n",
        "  print(\n",
        "    f\"Epoch {epoch:02d} | \"\n",
        "    f\"Train Loss: {train_loss:.4f} (coord {train_coord_loss:.4f}, alive {train_alive_loss:.4f}) | \"\n",
        "    f\"Val Loss: {val_loss:.4f} (coord {val_coord_loss:.4f}, alive {val_alive_loss:.4f})\"\n",
        "  )\n",
        "\n",
        "test_loss, test_coord_loss, test_alive_loss = evaluate(model, test_loader, device)\n",
        "print(\n",
        "  f\"Test Loss: {test_loss:.4f} (coord {test_coord_loss:.4f}, alive {test_alive_loss:.4f})\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXemL-TkzyK-",
        "outputId": "2195d080-f8a9-4205-8120-b86caad71d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 0.9676 (coord 0.8695, alive 0.1962) | Val Loss: 1.0170 (coord 0.9341, alive 0.1659)\n",
            "Epoch 02 | Train Loss: 0.7503 (coord 0.6608, alive 0.1789) | Val Loss: 0.8357 (coord 0.7554, alive 0.1605)\n",
            "Epoch 03 | Train Loss: 0.6308 (coord 0.5443, alive 0.1730) | Val Loss: 0.8050 (coord 0.7265, alive 0.1570)\n",
            "Epoch 04 | Train Loss: 0.6016 (coord 0.5169, alive 0.1694) | Val Loss: 0.7646 (coord 0.6881, alive 0.1530)\n",
            "Epoch 05 | Train Loss: 0.5773 (coord 0.4943, alive 0.1661) | Val Loss: 0.7634 (coord 0.6878, alive 0.1512)\n",
            "Epoch 06 | Train Loss: 0.5665 (coord 0.4847, alive 0.1635) | Val Loss: 0.7541 (coord 0.6764, alive 0.1554)\n",
            "Epoch 07 | Train Loss: 0.5611 (coord 0.4803, alive 0.1615) | Val Loss: 0.7605 (coord 0.6853, alive 0.1504)\n",
            "Epoch 08 | Train Loss: 0.5552 (coord 0.4750, alive 0.1605) | Val Loss: 0.7467 (coord 0.6728, alive 0.1477)\n",
            "Epoch 09 | Train Loss: 0.5513 (coord 0.4717, alive 0.1592) | Val Loss: 0.7303 (coord 0.6539, alive 0.1529)\n",
            "Epoch 10 | Train Loss: 0.5469 (coord 0.4676, alive 0.1586) | Val Loss: 0.7656 (coord 0.6900, alive 0.1512)\n",
            "Epoch 11 | Train Loss: 0.5421 (coord 0.4634, alive 0.1575) | Val Loss: 0.7424 (coord 0.6691, alive 0.1465)\n",
            "Epoch 12 | Train Loss: 0.5392 (coord 0.4611, alive 0.1563) | Val Loss: 0.7442 (coord 0.6697, alive 0.1491)\n",
            "Epoch 13 | Train Loss: 0.5354 (coord 0.4576, alive 0.1556) | Val Loss: 0.7400 (coord 0.6657, alive 0.1485)\n",
            "Epoch 14 | Train Loss: 0.5337 (coord 0.4561, alive 0.1553) | Val Loss: 0.7330 (coord 0.6590, alive 0.1479)\n",
            "Epoch 15 | Train Loss: 0.5302 (coord 0.4532, alive 0.1539) | Val Loss: 0.7331 (coord 0.6592, alive 0.1478)\n",
            "Epoch 16 | Train Loss: 0.5268 (coord 0.4502, alive 0.1533) | Val Loss: 0.7358 (coord 0.6607, alive 0.1502)\n",
            "Epoch 17 | Train Loss: 0.5242 (coord 0.4479, alive 0.1524) | Val Loss: 0.7307 (coord 0.6555, alive 0.1505)\n",
            "Epoch 18 | Train Loss: 0.5216 (coord 0.4457, alive 0.1517) | Val Loss: 0.7307 (coord 0.6564, alive 0.1488)\n",
            "Epoch 19 | Train Loss: 0.5176 (coord 0.4419, alive 0.1513) | Val Loss: 0.7263 (coord 0.6536, alive 0.1453)\n",
            "Epoch 20 | Train Loss: 0.5164 (coord 0.4412, alive 0.1504) | Val Loss: 0.7293 (coord 0.6551, alive 0.1483)\n",
            "Epoch 21 | Train Loss: 0.4864 (coord 0.4133, alive 0.1461) | Val Loss: 0.7158 (coord 0.6428, alive 0.1460)\n",
            "Epoch 22 | Train Loss: 0.4843 (coord 0.4116, alive 0.1454) | Val Loss: 0.7125 (coord 0.6397, alive 0.1457)\n",
            "Epoch 23 | Train Loss: 0.4827 (coord 0.4103, alive 0.1448) | Val Loss: 0.7180 (coord 0.6445, alive 0.1471)\n",
            "Epoch 24 | Train Loss: 0.4795 (coord 0.4073, alive 0.1443) | Val Loss: 0.7183 (coord 0.6452, alive 0.1463)\n",
            "Epoch 25 | Train Loss: 0.4786 (coord 0.4066, alive 0.1439) | Val Loss: 0.7216 (coord 0.6481, alive 0.1470)\n",
            "Epoch 26 | Train Loss: 0.4758 (coord 0.4042, alive 0.1431) | Val Loss: 0.7179 (coord 0.6443, alive 0.1472)\n",
            "Epoch 27 | Train Loss: 0.4741 (coord 0.4027, alive 0.1429) | Val Loss: 0.7191 (coord 0.6456, alive 0.1471)\n",
            "Epoch 28 | Train Loss: 0.4731 (coord 0.4018, alive 0.1424) | Val Loss: 0.7213 (coord 0.6470, alive 0.1485)\n",
            "Epoch 29 | Train Loss: 0.4714 (coord 0.4005, alive 0.1418) | Val Loss: 0.7211 (coord 0.6465, alive 0.1491)\n",
            "Epoch 30 | Train Loss: 0.4691 (coord 0.3984, alive 0.1414) | Val Loss: 0.7273 (coord 0.6529, alive 0.1487)\n",
            "Epoch 31 | Train Loss: 0.4687 (coord 0.3982, alive 0.1409) | Val Loss: 0.7206 (coord 0.6466, alive 0.1480)\n",
            "Epoch 32 | Train Loss: 0.4661 (coord 0.3961, alive 0.1402) | Val Loss: 0.7230 (coord 0.6490, alive 0.1480)\n",
            "Epoch 33 | Train Loss: 0.4644 (coord 0.3945, alive 0.1397) | Val Loss: 0.7238 (coord 0.6478, alive 0.1520)\n",
            "Epoch 34 | Train Loss: 0.4614 (coord 0.3919, alive 0.1391) | Val Loss: 0.7263 (coord 0.6516, alive 0.1494)\n",
            "Epoch 35 | Train Loss: 0.4611 (coord 0.3917, alive 0.1388) | Val Loss: 0.7320 (coord 0.6555, alive 0.1530)\n",
            "Epoch 36 | Train Loss: 0.4579 (coord 0.3889, alive 0.1381) | Val Loss: 0.7297 (coord 0.6538, alive 0.1518)\n",
            "Epoch 37 | Train Loss: 0.4564 (coord 0.3875, alive 0.1378) | Val Loss: 0.7382 (coord 0.6624, alive 0.1518)\n",
            "Epoch 38 | Train Loss: 0.4546 (coord 0.3859, alive 0.1373) | Val Loss: 0.7317 (coord 0.6562, alive 0.1509)\n",
            "Epoch 39 | Train Loss: 0.4524 (coord 0.3840, alive 0.1367) | Val Loss: 0.7487 (coord 0.6744, alive 0.1485)\n",
            "Epoch 40 | Train Loss: 0.4513 (coord 0.3832, alive 0.1363) | Val Loss: 0.7372 (coord 0.6621, alive 0.1502)\n",
            "Epoch 41 | Train Loss: 0.4488 (coord 0.3811, alive 0.1353) | Val Loss: 0.7347 (coord 0.6599, alive 0.1497)\n",
            "Epoch 42 | Train Loss: 0.4469 (coord 0.3795, alive 0.1348) | Val Loss: 0.7370 (coord 0.6607, alive 0.1526)\n",
            "Epoch 43 | Train Loss: 0.4447 (coord 0.3777, alive 0.1340) | Val Loss: 0.7469 (coord 0.6708, alive 0.1524)\n",
            "Epoch 44 | Train Loss: 0.4434 (coord 0.3766, alive 0.1338) | Val Loss: 0.7415 (coord 0.6655, alive 0.1520)\n",
            "Epoch 45 | Train Loss: 0.4409 (coord 0.3744, alive 0.1331) | Val Loss: 0.7423 (coord 0.6657, alive 0.1532)\n",
            "Epoch 46 | Train Loss: 0.4386 (coord 0.3724, alive 0.1325) | Val Loss: 0.7460 (coord 0.6699, alive 0.1522)\n",
            "Epoch 47 | Train Loss: 0.4361 (coord 0.3703, alive 0.1317) | Val Loss: 0.7472 (coord 0.6697, alive 0.1550)\n",
            "Epoch 48 | Train Loss: 0.4341 (coord 0.3686, alive 0.1311) | Val Loss: 0.7498 (coord 0.6723, alive 0.1548)\n",
            "Epoch 49 | Train Loss: 0.4337 (coord 0.3685, alive 0.1303) | Val Loss: 0.7464 (coord 0.6709, alive 0.1509)\n",
            "Epoch 50 | Train Loss: 0.4312 (coord 0.3663, alive 0.1299) | Val Loss: 0.7480 (coord 0.6717, alive 0.1526)\n",
            "Test Loss: 0.7282 (coord 0.6431, alive 0.1703)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# storm alive status accuracy\n",
        "@torch.no_grad()\n",
        "def alive_accuracy_per_horizon(model, loader, device, forecast_steps=4):\n",
        "  model.eval()\n",
        "  correct = torch.zeros(forecast_steps)\n",
        "  total = torch.zeros(forecast_steps)\n",
        "\n",
        "  for X_batch, y_coords_batch, y_alive_batch in loader:\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_alive_batch = y_alive_batch.to(device)\n",
        "\n",
        "    _, alive_logits = model(X_batch)\n",
        "    alive_pred = (torch.sigmoid(alive_logits) > 0.5).float()\n",
        "\n",
        "    for h in range(forecast_steps):\n",
        "      correct[h] += (alive_pred[:, h] == y_alive_batch[:, h]).sum().item()\n",
        "      total[h] += y_alive_batch[:, h].numel()\n",
        "\n",
        "  acc_per_h = (correct / total).tolist()\n",
        "  return acc_per_h\n",
        "\n",
        "# haversine formula to calculate distance between two points on sphere\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "  lat1, lon1, lat2, lon2 = map(torch.deg2rad, [lat1, lon1, lat2, lon2])\n",
        "  dlat = lat2 - lat1\n",
        "  dlon = lon2 - lon1\n",
        "  a = torch.sin(dlat/2)**2 + torch.cos(lat1)*torch.cos(lat2)*torch.sin(dlon/2)**2\n",
        "  c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
        "  return 6371.0 * c  # km\n",
        "\n",
        "# coordinate accruacy (by error)\n",
        "@torch.no_grad()\n",
        "def coordinate_error_km_per_horizon_delta(model, loader, device,\n",
        "                                          forecast_steps=4,\n",
        "                                          only_when_alive=True):\n",
        "  model.eval()\n",
        "  total_dist = torch.zeros(forecast_steps)\n",
        "  total_points = torch.zeros(forecast_steps)\n",
        "\n",
        "  for X_batch, y_deltas_batch, y_alive_batch in loader:\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_deltas_batch = y_deltas_batch.to(device)\n",
        "    y_alive_batch  = y_alive_batch.to(device)\n",
        "\n",
        "    deltas_pred, _ = model(X_batch)\n",
        "\n",
        "    lat_last = X_batch[:, -1, 0] * feat_std[0, 0, 0] + feat_mean[0, 0, 0]\n",
        "    lon_last = X_batch[:, -1, 1] * feat_std[0, 0, 1] + feat_mean[0, 0, 1]\n",
        "\n",
        "    for h in range(forecast_steps):\n",
        "      # predicted deltas\n",
        "      dlat_pred = deltas_pred[:, h, 0]\n",
        "      dlon_pred = deltas_pred[:, h, 1]\n",
        "\n",
        "      # true deltas\n",
        "      dlat_true = y_deltas_batch[:, h, 0]\n",
        "      dlon_true = y_deltas_batch[:, h, 1]\n",
        "\n",
        "      # convert deltas back to coordinates\n",
        "      lat_pred = lat_last + dlat_pred\n",
        "      lon_pred = lon_last + dlon_pred\n",
        "\n",
        "      lat_true = lat_last + dlat_true\n",
        "      lon_true = lon_last + dlon_true\n",
        "\n",
        "      dist = haversine(lat_pred, lon_pred, lat_true, lon_true)\n",
        "\n",
        "      if only_when_alive:\n",
        "        mask = (y_alive_batch[:, h] > 0.5)\n",
        "        if mask.sum() == 0:\n",
        "          continue\n",
        "        dist = dist[mask]\n",
        "\n",
        "      total_dist[h]  += dist.sum().item()\n",
        "      total_points[h] += dist.numel()\n",
        "\n",
        "  mean_err = (total_dist / total_points).tolist()\n",
        "  return mean_err\n",
        "\n",
        "acc_test_h  = alive_accuracy_per_horizon(model, test_loader, device)\n",
        "err_test_h = coordinate_error_km_per_horizon_delta(model, test_loader, device)\n",
        "print(\"Alive Status Accuracy: \", acc_test_h)\n",
        "print(\"Coordinate Error (km):\", err_test_h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkwjJO4t2olr",
        "outputId": "476e98cd-0b90-4b67-b663-20de7e00f8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alive accuracy per horizon (test):  [1.0, 0.9500328898429871, 0.9013806581497192, 0.8685075640678406]\n",
            "Coord error per horizon (km) â€” test (delta targets): [36.48537063598633, 79.68401336669922, 129.06336975097656, 182.84686279296875]\n"
          ]
        }
      ]
    }
  ]
}